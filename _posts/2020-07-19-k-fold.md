---
layout: post
title: Motivando k-Fold
featured-img: coverkfold
category: [üáßüá∑, basic]
mathjax: true
summary: Avalia√ß√£o e sele√ß√£o de modelos com um exemplo visual para regress√£o polinomial.
---

**Em constru√ß√£o: pequenos ajustes de formata√ß√£o do LaTeX e exemplos de c√≥digo**

Vamos discutir a motiva√ß√£o da avalia√ß√£o de modelos e dar ideias de como faz√™-la. Em seguida, daremos um exemplo de como a variabilidade da amostra pode atrapalhar no c√°lculo das estat√≠sticas de erro quando queremos otimizar hiper-par√¢metros. E, por fim, vamos apresentar a valida√ß√£o com o m√©todo *k-Fold* como alternativa, minimizando a vari√¢ncia.

## Avaliando a generaliza√ß√£o de um modelo

Investigar qual √© o desempenho real do nosso modelo em dados nunca vistos √© uma tarefa importante e dif√≠cil, que deve ser feita **minimizando os efeitos particulares da amostra que temos**. Se queremos criar um modelo para fazer previs√µes, n√£o adianta que a fun√ß√£o hip√≥tese tenha √≥timos resultados nos dados que s√£o utilizados no treinamento, se, quando dados novos surgirem, o modelo falhar em sugerir a vari√°vel alvo.

Vamos estudar esse efeito com um exemplo de regress√£o. Suponha que tenhamos o vetor aleat√≥rio $(X,Y)$ de forma que $X\sim\textrm{Uniforme}([-2,2])$ e

$$
\begin{equation*}
Y\sim 0.6X^3-2X^2-X+2+\varepsilon,
\end{equation*}
$$

em que o ru√≠do $\varepsilon$ √© uma vari√°vel aleat√≥ria tal que $\varepsilon\sim \mathcal{N}(0,0.1)$.

```python
def f(X):
    return np.power(x, 3)*0.6 - np.power(x, 2) - x + 2

def f_ruido(X):
    '''
    input
    - X: amostra da vari√°vel explicativa X (numpy array)
    output
    - amostra associada a Y com o ru√≠do (float)
    '''
    return f(X) + np.random.normal(0, 0.1, size = X.shape[0])
    
def sample(n):
    '''
    input
    - n: tamanho da amostra (inteiro)
    output
    - retorna uma amostra da distribui√ß√£o conjunta (X,Y) (numpy array, numpy array)
    '''
    X = np.random.uniform(-2, 2, size=n)
    Y = f_ruido(X)
    return X.reshape(-1, 1), Y.reshape(-1, 1)
```

Peguemos uma pequena amostra com apenas $20$ exemplos que pode ser vista na Figura 1.

```
X, Y = sample(20)
```

<p><center><img src="{{ site.baseurl }}/assets/img/kfold/imagem1.jpg"></center>
<center><b>Figura 1</b>: Os pontos azuis s√£o nossas observa√ß√µes e em preto temos a curva que gerou os dados.</center></p>


Vamos aproximar nossos dados com uma regress√£o polinomial. Explicando este algoritmo brevemente: dado $n\in\mathbb{N}$, induzimos "novas" vari√°veis atrav√©s de $X$ da forma $X_i=X^i$ para $i\in\\{ 0, 1, \cdots, n \\}$. Aplicamos ent√£o uma regress√£o linear m√∫ltipla nas vari√°veis $X_i$ criadas (neste caso, sem o intercepto pois ele est√° representado por $X_0=1$), encontrando os coeficientes $a_i$ que minimizam a soma dos res√≠duos ao quadrado, ficando com o modelo

$$
\begin{equation*}
f(x) = a_0 + \sum_{i=1}^n a_i x^i.
\end{equation*}
$$

H√° um hiper-par√¢metro que precisamos determinar antes do treinamento: o grau $n$ do polin√¥mio. Isso gera toda uma fam√≠lia de fun√ß√µes hip√≥teses poss√≠veis, indexada pelo valor de $n$. Mas qual destas fun√ß√µes devemos escolher? Ingenuamente, poder√≠amos acreditar que um modelo com erro baixo no treino nos geraria um modelo com erro baixo quando testado em dados futuros. Vamos ver que este nem sempre √© o caso.

Nos dados anteriores, podemos fazer o polin√¥mio de grau $19$ usando o m√©todo discutido anteriormente. A menos de erros de arredondamento na resolu√ß√£o do sistema linear (e possivelmente azar caso tenhamos dois valores de $X$ id√™nticos) o polin√¥mio encontrado pelo algoritmo √© exatamente o [polin√¥mio interpolador de Lagrange](https://en.wikipedia.org/wiki/Lagrange_polynomial). Neste caso, o MSE √© rigorosamente 0, pois o nosso polin√¥mio passar√° por todos os dados. Temos seu gr√°fico na Figura 2.

```
pf = PolynomialFeatures(degree = 9, include_bias= True)
lr = LinearRegression(fit_intercept=False)
lr.fit(pf.fit_transform(X),Y)
```

<p><center><img src="{{ site.baseurl }}/assets/img/kfold/imagem2.jpg"></center>
<center><b>Figura 2</b>: O polin√¥mio interpolador (em vermelho) passa por todos os nossos exemplo, entretanto n√£o √© uma boa aproxima√ß√£o para a fun√ß√£o que gerou os dados.</center></p>


Este polin√¥mio tem (praticamente) erro zero, mas graficamente vemos que ele n√£o aprendeu o padr√£o dos dados, ele apenas decorou as observa√ß√µes que a gente tinha nos dados de treino. Para ver isso numericamente, podemos criar uma nova amostra da mesma distribui√ß√£o que gerou os dados de treino, prever seus valores de $Y$ utilizando o polin√¥mio e calcular o valor de $R^2$.

```python
X_new, Y_new = sample(100)
print('R^2 no treinamento: ',r2_score(Y, lr.predict(pf.fit_transform(X))))
print('R^2 em um novo conjunto: ', r2_score(Y_new, lr.predict(pf.fit_transform(X_new))))
```

```
R^2 no treinamento:  1.0
R^2 em um novo conjunto:  -12.241709591979532
```

Como o MSE no treinamento √© virtualmente 0, o $R^2$ do treino √© 1, nos dando a falsa impress√£o que temos um modelo perfeito e que encontramos exatamente a fun√ß√£o que descreve os dados. Mas nos exemplos novos gerados pela mesma fun√ß√£o, ficamos com um $R^2$ baix√≠ssimo de $-12.242$.

Claramente, temos _overfit_. O modelo n√£o aprendeu o padr√£o de cria√ß√£o dos dados, apenas decorou o que acontece nos dados de treino. Isso compromete a sua sua generaliza√ß√£o para observa√ß√µes desconhecidas.

## Sele√ß√£o de modelos com hold-out

A maneira que come√ßamos a resolver esse problema √© parecida com a forma como identificamos ele. Agora, ao inv√©s de usar todos os nossos dados iniciais para treinamento, vamos separa-los em duas partes: uma parte para treino e uma parte para valida√ß√£o. Essa maneira √© conhecida como valida√ß√£o _hold-out_.

Agora, usamos dados de treinamento para treinar nossos v√°rios modelos e em seguida, nos dados de teste, calculamos alguma m√©trica de erro. Como o processo de gera√ß√£o dos dados de treino e teste √© o mesmo, estamos avaliando o modelo em dados que tem o mesmo padr√£o dos exemplos de treinamento, mas n√£o s√£o os mesmos. Assim, conseguimos avaliar os modelos pelas m√©tricas, mas descartando modelos que apenas decoraram as respostas.

A ideia do *hold-out* na avalia√ß√£o de um modelo $\widehat{f}\_\alpha$, em que $\alpha\in \Omega$ √© uma escolha de hiper-par√¢metro entre todas as poss√≠veis do conjunto $\Omega$ (no nosso caso, $\alpha$ √© o grau do polin√¥mio e $\Omega \subset \mathbb{N}$) pode ser resumida como:

- Dividimos nossos dados em um conjunto de treino e um conjunto de teste.
- Para cada $\alpha\in\Omega$, treinamos o modelo com o conjunto de dados de treino. Em seguida avaliamos o modelo com o conjunto de dados de teste. O valor obtido √© ent√£o a nossa estimativa para erro do modelo $$\begin{equation*}\widehat{f}\_\alpha\end{equation*}$$ em dados n√£o vistos. Chamamos esta quantidade de $\widehat{E}\_\alpha$.
- Com as estimativas para o erro, avaliamos os pontos $\\{(\alpha, \widehat{E}\_\alpha): \alpha \in \Omega\\}$ e escolhemos o valor de $\alpha$ que mais nos agrade. Em muitos contextos √© comum escolher $\alpha^* = \arg\min\_{\alpha \in \Omega} \widehat{E}\_\alpha$. 

Essa abordagem de valida√ß√£o j√° resolve v√°rios dos nossos problemas. Mas ainda h√° um fator que pode nos atrapalhar. Ficamos dependentes de como separamos nossa amostras de treino e de teste. Uma quebra infeliz pode gerar um vi√©s indesejado nas estat√≠sticas calculadas.

Se a distribui√ß√£o dos dados de teste for muito diferente da distribui√ß√£o dos dados de treinamento, podemos beneficiar ou atrapalhar o desempenho de alguns dos modelos. Nos dados anteriores, imagine que tiv√©ssemos o modelo $A$ que √© muito bom para $X$ negativos, e muito ruim para $X$ positivos. E um modelo $B$ que funciona melhor que $A$ no geral, pois acerta bastante para $X$ qualquer, mas n√£o √© t√£o bom quanto $A$ para valores negativos. Neste caso, se nosso conjunto de teste privilegiasse valores negativos de $X$, poder√≠amos escolher o modelo $A$ sobre o modelo $B$, mesmo isso sendo pior na m√©dia. Quando a distribui√ß√£o de teste e treino s√£o diferentes, temos um problema de *train/test missmatch*.

Para ilustrar essa variabilidade de acordo com a quebra que fazemos, vamos simular v√°rios ```train_test_split``` diferentes nos nossos dados, fazendo o procedimento discutido anteriormente para escolher o grau do polin√¥mio da regress√£o por m√≠nimos quadrados.

Estamos apenas alterando a forma como quebramos entre treino e teste (variando a ```random_state``` do ```train_test_split```). O conjunto de dados n√£o muda. Ainda assim, isso √© suficiente para termos curvas diferentes para cada quebra como vemos na Figura 3.

```python
def varios_cortes_train_test(X, Y, quantidade_cortes = 5, grau_maximo = 7):
    """
    nilvo
    """
    dic = {'quebra': [], 'grau': [], 'MAE': [], 'MSE': [], 'R2': []}
    for quebra in range(1, quantidade_cortes + 1):
        for grau in range(0, grau_maximo + 1):
            pf = PolynomialFeatures(degree =g rau)
            X_ = pf.fit_transform(X)
            
            X_train, X_test, y_train, y_test = train_test_split(X_, Y, shuffle = True, test_size = 0.2, random_state = quebra)
            lr = LinearRegression()
            lr.fit(X_train, y_train)
            
            dic['quebra'].append(quebra)
            dic['grau'].append(grau)
            dic['MAE'].append(mean_absolute_error(y_test,lr.predict(X_test)))
            dic['MSE'].append(mean_squared_error(y_test,lr.predict(X_test)))
            dic['R2'].append(r2_score(y_test,lr.predict(X_test)))
    
    df = pd.DataFrame(dic)
    plot_metricas_por_grau(df, quantidade_cortes)
    
def plot_metricas_por_grau(df, quantidade_cortes, metrica_list = ['MSE','MAE','R2']):
    """
    wtf
    """
    plt.figure(figsize=(16, 4))
    for j, metrica in zip(range(1, len(metrica_list) + 1), metrica_list):
        plt.subplot(1, len(metrica_list), j)
        for i, c in zip(range(1, quantidade_cortes + 1), cycle('bgrcmyk')):
            plt.plot(df['grau'][df['quebra'] == i], df[metrica][df['quebra'] == i], color = c)
        plt.ylabel(metrica)
        plt.xlabel('grau do polin√¥mio')
    plt.savefig('imagem3.jpg', bbox_inches = 'tight')
    plt.show()
```

<p><center><img src="{{ site.baseurl }}/assets/img/kfold/imagem3.jpg"></center>
<center><b>Figura 3</b>: Os pontos azuis s√£o nossas observa√ß√µes e em preto temos a curva que gerou os dados.</center></p>

## Sele√ß√£o de modelos com k-Fold

A ideia aqui √© usar todos os dados poss√≠veis no treino e no teste, minimizando a vari√¢ncia associada com a quebra entre treino e teste. Vamos discutir a ideia do algoritmo *k-Fold* na otimiza√ß√£o de hiper-par√¢metros como fizemos na apresenta√ß√£o do *hold-out*.

- Dividimos nossos dados em $ k$ conjuntos de (aproximadamente) mesmo tamanho, tamb√©m chamados de pastas $1, 2, 3, \cdots, k$.

- Fixado  $i\in\\{1,2,\dots,k\\}$ e $\alpha \in \Omega$, treinamos o modelo $\widehat{f}\_\alpha^{\,i}$ nas pastas $1,2,\cdots,i-1,i+1,\cdots,k$ e medimos suas m√©tricas de avalia√ß√£o na pasta $i$, obtendo o erro $E\_\alpha^{\,i}$. Este procedimento √© feito para cada $i\in\\{1,2,\dots,k\\}$ e para cada $\alpha\in\Omega$.

- Por fim, valor estimado para erro do modelo $\widehat{f}_\alpha$ para dados n√£o vistos √© dado por

  $$
  \begin{equation*}
  \widehat{E}_\alpha = \sum_{i=1}^k E_\alpha^{\, i}.
  \end{equation*}
  $$

- Com essas estimativas para o erro, avaliamos os pontos $\\{(\alpha, \widehat{E}\_\alpha): \alpha \in \Omega\\}$ e escolhemos o valor de $\alpha$ que mais nos agrade. Em muitos contextos podemos escolher $\alpha^* = \arg\min\_{\alpha \in \Omega} \widehat{E}\_\alpha$. 

No nosso exemplo, fazer a valida√ß√£o com o *k-Fold*, com $ k=5$, nos d√° estimativas muito menos vari√°veis quando mudamos o ```random_state``` como podemos ver na Figura 4.

```python
def varios_cortes_cross_validate(X, Y, quantidade_cortes = 10, grau_maximo = 7):
    """
    nilvo
    """
    dic = {'quebra': [], 'grau': [], 'MAE': [], 'MSE': [], 'R2': []}
    for quebra in range(1, quantidade_cortes + 1):
        kfold = KFold(n_splits = 5, shuffle = True, random_state = quebra)
        for grau in range(0, grau_maximo + 1):
            pf = PolynomialFeatures(degree = grau)
            X_ = pf.fit_transform(X)
            
            aux = pd.DataFrame(cross_validate(lr, X_, Y, scoring = ['r2', 'neg_mean_squared_error', 'neg_mean_absolute_error'], cv = kfold, return_train_score = False))
            dic['quebra'].append(quebra)
            dic['grau'].append(grau)
            dic['MAE'].append(-aux.mean()['test_neg_mean_absolute_error'])
            dic['MSE'].append(-aux.mean()['test_neg_mean_squared_error'])
            dic['R2'].append(aux.mean()['test_r2'])
    
    df = pd.DataFrame(dic)
    plot_metricas_por_grau(df, quantidade_cortes)
```

<p><center><img src="{{ site.baseurl }}/assets/img/kfold/imagem4.jpg"></center>
<center><b>Figura 4</b>: Os pontos azuis s√£o nossas observa√ß√µes e em preto temos a curva que gerou os dados.</center></p>

Neste caso, temos estimativas muito mais seguras para o valor do erro em cada valor do grau do polin√¥mio e n√£o precisamos de mais dados para isso. Precisamos apenas de uma maneira mais inteligente de validar nosso modelo.

Uma d√∫vida que pode surgir √©: porque n√£o fazer $k$ valida√ß√µes *hold-out* tradicionais? Porque fazer a quebra em pastas? A divis√£o em pastas nos garante que todos os dados est√£o sendo usados para teste, isso nos deixa mais seguros minimizando o efeito de *train/test missmatch*. 

Comentei que dependo do problema podemos escolher $\alpha^*\in\Omega$ de maneira diferente. Por exemplo, no nosso caso, √© poss√≠vel optar pelo grau do polin√¥mio ideal como sendo $2$ ou $3$, valores em que temos os maiores ganhos de redu√ß√£o da m√©trica. A ideia neste caso seria que simplicidade tamb√©m √© importante, ent√£o mesmo que o grau do polin√¥mio que teria o menor erro seja alto, priorizamos modelos menos complicados.

## Observa√ß√£o complementar a respeito de aproxima√ß√µes polinomiais

O [Teorema de Stone-Weierstrass](https://pt.wikipedia.org/wiki/Teorema_de_Stone-Weierstrass) nos garante que, para qualquer fun√ß√£o real cont√≠nua $g:I\to\mathbb{R}$ tal que o dom√≠nio $I$ √© um intervalo fechado (mais geralmente, um compacto), existe uma sequ√™ncia de polin√¥mios que converge [uniformemente]([https://pt.wikipedia.org/wiki/Converg%C3%AAncia_uniforme](https://pt.wikipedia.org/wiki/Converg√™ncia_uniforme)) para $g$. Isso nos garante que, dada uma toler√¢ncia $\delta>0$, existe um polin√¥mio $p$ de grau suficientemente grande tal que $\|g(x)-p(x)\|<\delta$ para todo $x\in I$. 

Portanto, a abordagem de aproxima√ß√£o polinomial pareceria muito promissora. Entretanto o Teorema de Stone-Weierstrass nos garante a exist√™ncia do polin√¥mio, mas n√£o nos da uma maneira construtiva de encontra-lo. Al√©m disso, o problema aqui √© diferente: n√£o sabemos a forma de $f$, temos apenas  o valor aproximado dela em alguns pontos. Estimar sua forma a partir de interpola√ß√£o polinomial nos gera polin√¥mios que n√£o se comportam bem fora de pontos amostrados quando aumentamos o n√∫mero de dados (e consequentemente o grau do polin√¥mio interpolador). Al√©m disso, o c√°lculo num√©rico carrega erros de arredondamento que geram resultados estranhos. Uma alternativa um pouco melhor √© utilizando [splines](https://en.wikipedia.org/wiki/Spline_(mathematics)).