---
layout: post
title: Generalizando dist√¢ncia
featured-img: coverdistancia
category: [üáßüá∑, math]
mathjax: true
summary: Defini√ß√£o matem√°tica de dist√¢ncia com aplica√ß√µes no contexto de ci√™ncia de dados.
---

<p><div align="justify">V√°rios algoritmos de aprendizado de m√°quina baseados em dist√¢ncia s√£o gen√©ricos o suficiente para mudarmos a forma como calculamos a dist√¢ncia entre dois pontos. Quando olhamos para dados em $\mathbb{R}^n$, para $n\in\mathbb{N}^*$, estamos acostumados com a <b>dist√¢ncia euclidiana</b>. Essa dist√¢ncia calcula o tamanho do comprimento de reta que liga os dois pontos, com uma esp√©cie de generaliza√ß√£o do teorema de Pit√°goras.</div></p>

<p><div align="justify">Explicitamente temos, para $\textbf{x} = (x_1, x_2, \cdots, x_n) \in \mathbb{R}^n$ e $\textbf{y} = (y_1, y_2, \cdots, y_n) \in \mathbb{R}^n$, a dist√¢ncia dada por</div></p>

$$
\begin{equation*}
 \textrm{dist√¢ncia euclidiana entre }\textbf{x}\textrm{ e }\textbf{y} = \sqrt{ \sum_{i=1}^{n} |x_i-y_i|^2  } .
\end{equation*}
$$

<p><div align="justify">Entretanto, dependendo da natureza do problema essa dist√¢ncia pode n√£o ser a mais indicada. Neste post, vamos conversar sobre a defini√ß√£o de dist√¢ncia aos olhos de conceitos b√°sicos de topologia de espa√ßos m√©tricos e exemplificar algumas m√©tricas cl√°ssicas entendendo a diferen√ßa entre elas. Essa discuss√£o pode ser importante para se aprofundar em algoritmos cl√°ssicos que utilizam um c√°lculo de dist√¢ncia como o kNN, o DBScan e o k-means e suas varia√ß√µes. Al√©m de aprender a identificar outros momentos em que voc√™ pode utilizar o conceito de dist√¢ncia.</div></p>

#  Definindo formalmente a dist√¢ncia entre dois pontos

<p><div align="justify">Intuitivamente, uma dist√¢ncia precisa satisfazer algumas propriedades que surgem da forma como vemos dist√¢ncia intuitivamente. Iremos ver uma defini√ß√£o matem√°tica para ela que tenta sintetizar essas no√ß√µes em termos matematicamente claros.</div></p>

<p><div align="justify">Primeiro, √© razo√°vel pedir <i>simetria</i>: a dist√¢ncia de $x$ at√© $y$ seja igual √† dist√¢ncia de $y$ at√© $x$. Isso parece √≥bvio, mas a nossa defini√ß√£o formal de dist√¢ncia ser√° uma fun√ß√£o que aceita duas entradas e retorna um valor, que chamaremos de dist√¢ncia entre os argumentos de entrada. Essa primeria propriedade desejada nos dir√° que n√£o importa a ordem que damos as entradas.</div></p>

<p><div align="justify">Uma outra propriedade desej√°vel √© a <i>identidade</i>: a dist√¢ncia de um ponto at√© ele mesmo √© zero e se dois pontos est√£o a uma dist√¢ncia zero entr√£o eles s√£o o mesmo elemento. Isso tamb√©m √© bem razo√°vel e nos diz que apenas o pr√≥prio ponto tem dist√¢ncia zero dele mesmo.</div></p>

<p><div align="justify">Por fim, no ensino fundamental nos dizem que, dado um tri√¢ngulo, ent√£o a soma do comprimento de dois lados sempre √© maior ou igual que o comprimento do lado restante para o tri√¢ngulo ser v√°lido. Vamos querer manter essa propriedade na nossa defini√ß√£o formal de dist√¢ncia, chamando essa propriedade de <i>desigualdade triangular</i>.</div></p>

<p><div align="justify">Com essa no√ß√£o intuitiva de dist√¢ncia, criamos a formaliza√ß√£o dada pela defini√ß√£o matem√°tica:</div></p>

<p><div align="justify"><b>Defini√ß√£o</b>: Dado um conjunto $\mathcal{A}$, uma fun√ß√£o $d:\mathcal{A}\times\mathcal{A}\to \mathbb{R}$ √© chamada de uma <b>m√©trica</b> (ou <b>dist√¢ncia</b>) em $\mathcal{A}$ se, dados $x,y,z\in\mathcal{A}$ quaisquer, satisfaz:</div></p>

* $d(x,y) = 0 \Leftrightarrow x = y$ (identidade);
* $d(x,y) = d(y,x)$ (simetria);
* $d(x,y) + d(y,z) \geq d(x,z)$ (desigualdade triangular).

<p><div align="justify">Repare que dessas propriedades, tiramos ainda outras propriedades desejadas, como, por exemplo, a <i>n√£o-negatividade</i>. Seria contraintuitivo medir a dist√¢ncia entre dois pontos e obter um n√∫mero negativo. Para mostrar que isso vale no nosso caso, ou seja, que $d(x,y)\geq 0$ para quaisquer $x,y\in \mathcal{A}$. Pela desigualdade triangular, $d(x,y) + d(y,x) \geq d(x,x)$. Pela simetria e usando que $d(x,x)=0$ temos que $2 \, d(x,y) \geq 0$, e conclu√≠mos o desejado.</div></p>

<p><div align="justify">Fazemos com que, uma fun√ß√£o m√©trica $d$ definida como anteriormente, a <b>dist√¢ncia entre dois pontos</b> $x,y\in \mathcal{A}$ √© dada por $d(x,y)$.</div></p>

#  Exemplos cl√°ssicos para $\mathbb{R}^n$

<p><div align="justify">A natureza e escolha da m√©trica varia de acordo com o problema estudado. Em geral, quando $\mathcal{A}=\mathbb{R}^n$, estamos interessados em <b>dist√¢ncias induzidas pelas normas Lp's</b> (com $1\leq p \leq \infty$) dadas na forma</div></p>

$$
\begin{equation*}
d_p(\textbf{x},\textbf{y}) = || \textbf{x} - \textbf{y} ||_p, 
\end{equation*}
$$

<p><div align="justify">com soma de vetores coordenada a coordenada em que a norma Lp $||\cdot||_p : \mathbb{R}^n \to \mathbb{R} $ √© dada por</div></p>

$$
\begin{equation*}
 ||\textbf{x}||_p = \left( \sum_{i=1}^{n} |x_i|^p  \right)^{\frac{1}{p}}.
\end{equation*}
$$

<p><div align="justify">As m√©tricas dessa fam√≠lia de dist√¢ncias, no contexto de aprendizado de m√°quina, s√£o mais conhecidas como <b>dist√¢ncia Minkowski com par√¢metro p</b>. Repare que a dist√¢ncia euclidiana usual √© a dist√¢ncia de Minkowski com par√¢metro 2. O caso limite, quando $p=\infty$ √© definido como o maior valor absoluto entre as coordenadas, ou seja,</div></p>

$$
\begin{equation*}
||\textbf{x}||_{\infty} = \max_{1\,\leq \,i \,\leq \,n} |x_i|.
\end{equation*}
$$

<p><div align="justify">A m√©trica $d_\infty$ √© tamb√©m conhecida como <b>dist√¢ncia de Chebyshev</b> ou <b>do m√°ximo</b>. Um outro nome cl√°ssico para a m√©trica $d_1$ √© <b>dist√¢ncia de Manhattan</b>.</div></p>

##  Diferentes bolas do $\mathbb{R}^n$

<p><div align="justify">Para ilustrar como essa diferentes formas de medir dist√¢ncia funcionam, vamos definir um conceito primordial de topologia de espa√ßos m√©tricos: a bola aberta. A no√ß√£o de bola tenta dar um significado para a pergunta: <i>"O que significa ter um elemento perto de outro?"</i>.</div></p>

<p><div align="justify">Primeiro, teremos um par√¢metro relacionado com o ponto central de compara√ß√£o. Este ser√° o elemento com o qual compararemos os outros, tentando responder se est√£o pr√≥ximos ou n√£o. Al√©m disso, o significado de perto depende da nossa toler√¢ncia: duas pessoas sentadas a menos de 1 metro √© perto (ainda mais em √©poca de corona v√≠rus), mas um meteoro a 1 quil√¥metro da terra tamb√©m √© perto aos olhos de um astr√¥nomo. A nossa bola tamb√©m ter√° um par√¢metro, que chamaremos de <i>raio</i>, que nos dar√° at√© quanto estamos considerando perto.</div></p>

<p><div align="justify"><b>Defini√ß√£o:</b> Seja $d$ uma m√©trica em um conjunto $\mathcal{A}$. Uma <b>bola aberta</b> de raio $r>0$ centrada no ponto $x\in \mathcal{A}$ √© o conjunto</div></p>

$$
B_d(x;r) = \{ y\in \mathcal{A} : d(x,y) \lt r\}.
$$

<p><div align="justify">Os elementos de $B_d(x;r)$ s√£o os elementos de $\mathcal{A}$ perto de $x$ (sob essa toler√¢ncia de raio $r$). Essa no√ß√£o de perto, formalizada pela bola, ajuda a definir o significado de converg√™ncia e continuidade em espa√ßos abstratos, mas n√£o entraremos nesses t√≥picos.</div></p>

<p><div align="justify">Vamos brincar com o formato dessas bolas quando $\mathcal{A}=\mathbb{R}^2$ e $d=d_p$, a dist√¢ncia de Minkwoski de par√¢metro $p$, variando o valor do $p$. Essas m√©tricas j√° est√£o implementadas no <a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html"><code>sklearn.neighbors.DistanceMetric</code></a>, vamos apenas criar uma fun√ß√£o que recebe uma m√©trica, um raio e um centro, e plota a bola associada.</div></p>

```python
import sklearn.neighbors as neigh

def bola_aberta(dist_list, raio = 1, centro = [0,0]):
    
    """
    dist: lista de dist√¢ncias (list of sklearn.neighbors.DistanceMetric functions)
    raio: raio das bolas (positive float), pode ser passado tamb√©m como lista de raios (list of positives floats)
    centro da bola: lista com posi√ß√£o do centro da bola (x_centro,y_centro) (list of floats, len = 2)
    
    return: plots das bolas de centro "centro" das m√©tricas associadas aos respectivos raios (at√© tr√™s imagens por fileira).
            eixos sempre entre -1.5 e 1.5.
    """
    
    if type(raio) in [int, float]:
        raio = [raio]*len(dist_list)
    
    # criando os pontos da malha para fazer as curvas de n√≠vel da fun√ß√£o
    # indicadora que nos da se est√° dentro ou n√£o da bola
    x_vals = np.linspace(-1.5,1.5, 400)
    y_vals = np.linspace(-1.5,1.5, 400)
    X, Y = np.meshgrid(x_vals, y_vals)
    
    n = len(dist_list)
    m = np.ceil(len(dist_list)/3)
    plt.figure(figsize=(12, 4*m))
    
    for j, dist in zip(range(1,  n + 1), dist_list):
        plt.subplot(m, 3, j)        
        
        # fun√ß√£o indicadora (estou dentro da bola ou n√£o? 1 se sim, 0 se n√£o)
        Z = np.asarray([[1 if dist.pairwise(np.array([[x,y]]),np.array([centro]))[0,0] < raio[j-1] else 0 for x in x_vals] for y in y_vals])
        cp = plt.contourf(X, Y, Z, levels = [-0.1,0.1,0.9, 1.1], cmap=cmap)
        plt.xticks([-1,0,1])
        plt.yticks([-1,0,1])
#     plt.show()
```

<p><div align="justify">Os plots da Figura 1 mostram como varia o formato da bola centrada na origem e de raio 1 para $p = 1$, $1.5$, $2$, $3$, $10$, $\infty$, em ordem.</div></p>

```python
lista_distancias = []
for p_ in [1, 1.5, 2, 3, 10]:
    lista_distancias.append(neigh.DistanceMetric.get_metric('minkowski', p = p_))
lista_distancias.append(neigh.DistanceMetric.get_metric('chebyshev'))

bola_aberta(lista_distancias)
```

<p><center><img src="{{ site.baseurl }}/assets/img/distancia/imagem1.jpg"></center>
<center><b>Figura 1</b>: Formato de bolas do plano para diferentes valores de p da m√©trica de Minkowski. A regi√£o cinza √© o lado de fora da bola e a regi√£o vermelha √© o lado de dentro.</center></p>

<p><div align="justify">A borda da nossa bola n√£o est√° inclu√≠da nela, da√≠ o nome aberta, an√°logo aos intervalos abertos da reta que n√£o cont√©m os elementos da fronteira (de fato, um intervalo aberto √© uma bola aberta na reta). De toda forma √© interessante analisar a borda para entender seus diferentes formatos. Como temos $r=0$ e o centro como sendo a origem, vale que a borda √© dada pelos valores $(x_1,x_2)\in\mathbb{R}^2$ que satisfazem a equa√ß√£o</div></p>

$$
\begin{equation*}
|x_1|^p + |x_2|^p = 1.
\end{equation*}
$$

<p><div align="justify">Quando $p=2$, temos o c√≠rculo, da nossa no√ß√£o euclidiana de bola. Mas repara que quando $p=1$, temos quatro reta, dependendo do sinal de $x_1$ e $x_2$ e por isso obtemos o losango. O caso $p=\infty$ √© dado por</div></p>

$$
\begin{equation*}
\max\{|x_1|, |x_2|\} = 1,
\end{equation*}
$$

<p><div align="justify">e √© por isso que temos a borda nos valores dos pontos que tem $|x_1|=1$ ou $|x_2|=1$.</div></p>

<p><div align="justify">Eu nunca vou entender qual a gra√ßa do programa Chaves, mas imposs√≠vel n√£o fazer um coment√°rio infeliz sobre as bolas quadradas do Kiko estarem na verdade utilizando a m√©trica de Chebyshev. O Kiko, como voc√™ agora, entende bastante de topologia de espa√ßos m√©tricos.</div></p>

# Mais exemplos

<p><div align="justify">Vamos passar por mais alguns exemplos interessantes que podem ajudar a construir a intui√ß√£o ou que s√£o relevantes no contexto de ci√™ncia de dados.</div></p>

## M√©trica discreta

<p><div align="justify">Imagine um experimento ex√≥tico em que o valor expl√≠cito da dist√¢ncia entre dois pontos n√£o √© importante, mas √© relevante saber se dois elementos s√£o iguais ou n√£o. Nesse cen√°rio, a <b>m√©trica discreta</b> pode ser √∫til. Dado $\mathcal{A}$ qualquer, a dist√¢ncia $d_{\textrm{disc}}$ entre $x,y\in\mathcal{A}$ √© dada por</div></p>

$$
\begin{equation*}
d_{\textrm{disc}}(x,y)=
\begin{cases}
0 \textrm{, se }x=y,\\
1 \textrm{, caso contr√°rio.}
\end{cases}
\end{equation*}
$$

<p><div align="justify">√â um exerc√≠cio legal se convencer que esta forma de dist√¢ncia satisfaz as propriedades que desej√°vamos na defini√ß√£o de m√©trica.</div></p>

<p><div align="justify">Aqui, a no√ß√£o de perto ou distante se tornam um pouco contraintuitiva. Se $\mathcal{A}=\mathbb{R}^2$, ent√£o o ponto $(0,0)$ est√° a mesma dist√¢ncia do ponto $(0,1)$ e do ponto $(42,-42)$. Podemos olhar isso analisando as bolas para diferentes valores do raio $r$. Para qualquer $r\in (0,1]$, a temos que $B_{d_{\textrm{disc}}}(x;r) = \{ x \}$, pois somente $x$ est√° a uma dist√¢ncia menor que 1 dele mesmo. Agora, para qualquer $r\in (1,\infty)$ temos que $B_{d_{\textrm{disc}}}(x;r) = \mathcal{A}$ uma vez que qualquer ponto est√° a uma dist√¢ncia menor ou igual a 1 de $x$. </div></p>

<p><div align="justify">No <a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html"><code>sklearn.neighbors.DistanceMetric</code></a>, podemos passar uma m√©trica gen√©rica que respeite a defini√ß√£o que fizemos na defini√ß√£o formal. Usando o argumento <code>pyfunc</code> e estabelecendo a fun√ß√£o m√©trica em <code>func</code> que recebe dois vetores numpy unidimensionais e retorna a dist√¢ncia entre eles.</div></p>

```python
def discrete(X, Y):
    """
    X, Y: vetores que queremos calcular a dist√¢ncia discreta(np array of floats)
    return dist√¢ncia discreta entre o ponto X e Y
    """
    if np.all(X == Y):
        return 0
    else:
        return 1

bola_aberta([neigh.DistanceMetric.get_metric('pyfunc', func = discrete)]*2,[0.5, 1.5])
```

<p><div align="justify">Na Figura 2, passamos como raios das bolas os valores $0.5$ e $1.5$, para ver o efeito discutido anteriormente de que $B_{d_{\textrm{disc}}}((0,0);0.5)=\{(0,0)\}$ e $B_{d_{\textrm{disc}}}((0,0);2)=\mathbb{R}$.</div></p>

<p><center><img src="{{ site.baseurl }}/assets/img/distancia/imagem2.jpg"></center>
<center><b>Figura 2</b>: Bolas da m√©trica discreta. Lembrando que a regi√£o cinza √© o lado de fora da bola e a regi√£o vermelha √© o lado de dentro. Na primeira imagem, n√£o podemos ver que o ponto (0,0) est√° dentro da bola pela resolu√ß√£o.</center></p>

## Dist√¢ncia de Hamming

<p><div align="justify">A <b>dist√¢ncia de Hamming</b> entre dois vetores de tamanho $n$ √© o n√∫mero de componentes diferentes entre eles. Ela se demonstra muito √∫til quando o significado dos valores das componentes n√£o s√£o num√©ricos ou n√£o tem um sentido de ordem. Vamos dar um cen√°rio em que pode fazer sentido usar ela:</div></p>

### Dist√¢ncia entre panelas

<p><div align="justify">Imagine que eu queira descrever a dist√¢ncia entre duas panelas e os atributos que eu considero importante sobre esse objeto s√£o a cor e a presen√ßa ou n√£o de cabo. Neste caso, estamos representando nosso espa√ßo como $\mathcal{A} =$ $\{ \textrm{cinza}, \textrm{vermelha}, \textrm{preta}  \}$ $\times \{\textrm{tem cabo}, \textrm{n√£o tem cabo} \}$. Uma panela √© representada por um vetor de duas componentes, na primeira a sua cor e na segunda a presen√ßa de cabo, por exemplo $(\textrm{cinza}, \textrm{tem cabo})$. </div></p>

<p><div align="justify">A maneira como vimos dist√¢ncia at√© agora apenas falava sobre n√∫meros. Podemos adaptar $\mathcal{A}$ para analisar o espa√ßo $\mathcal{A}' =\{1,2,3\}\times\{1,2\}$ em que fazemos a bije√ß√£o entre os elementos pelo mapa ($\textrm{cinza}\to 1$, $\textrm{vermelha}\to 2$, $\textrm{preta}\to 3$) e ($\textrm{tem cabo}\to 1$, $\textrm{n√£o tem cabo}\to 2$). Com essa transforma√ß√£o, podemos usar por exemplo a dist√¢ncia de Minkowski com par√¢metro 1.</div></p>

<p><div align="justify">Imagine que temos $\textrm{panela}_1 = (\textrm{cinza},\textrm{tem cabo})$, $\textrm{panela}_2 = (\textrm{preta},\textrm{tem cabo})$, e $\textrm{panela}_3 = (\textrm{vermelha},\textrm{n√£o tem cabo})$. Neste caso, ficar√≠amos com coisas do tipo:</div></p>

$$
\begin{equation*}
d_{\textrm{panelas}}(\textrm{panela}_1, \textrm{panela}_2 ) = d_1((1,1),(3,1)) = |1-3|+|1-1| = 2,
\end{equation*}
$$

enquanto,

$$
\begin{equation*}
d_{\textrm{panelas}}(\textrm{panela}_1, \textrm{panela}_3 ) = d_1((1,1),(2,2)) = |1-2|+|1-2| = 2.
\end{equation*}
$$

<p><div align="justify">Esta forma de calcular dist√¢ncias, est√° nos falando que a $\textrm{panela}_1$ est√° a mesma dist√¢ncia da $\textrm{panela}_2$ e da $\textrm{panela}_3$. A $\textrm{panela}_1$ difere da $\textrm{panela}_2$ apenas pela cor, enquanto difere em cor e na presen√ßa de cabo da $\textrm{panela}_3$. O problema surgiu aqui porque levamos as cores pra reta como se esses objetos tivessem uma ordem, o que n√£o faz sentido aqui.</div></p>

<p><div align="justify">Uma alternativa √© justamente calcular a dist√¢ncia Hamming das panelas, somando $1$ na dist√¢ncia pra cada categoria (cor e presen√ßa de cabo) em que h√° diferen√ßa. Neste caso a dist√¢ncia de Hamming da $\textrm{panela}_1$ e a $\textrm{panela}_2$ seria 1 pois diferem apenas na primeira componente.</div></p>

<p><div align="justify">$\oint$ <i>A dist√¢ncia de Hamming √© equivalente a fazer um <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"><code>sklearn.preprocessing.OneHotEncoder</code></a> de cada uma das componentes dos vetores e depois calcular a dist√¢ncia com qualquer Minkowski (dividindo por 2 o resultado final).</i></div></p>

<p><div align="justify">Essa dist√¢ncia do √¢ngulo √© baseada na "dist√¢ncia do cosseno". Que diz que a dist√¢ncia entre dois vetores √© o cosseno do produto interno. Ela, de fato, n√£o √© uma dist√¢ncia pois n√£o satisfaz a defini√ß√£o matem√°tica de m√©trica. Entretanto, isso n√£o a torna descart√°vel. Em muitos casos, uma medida de (dis)similaridade entre pontos - que define o qu√£o parecido ou diferente eles s√£o - j√° √© suficiente para resolver o problema. Em outros casos, ser√° importante satisfazer todas as defini√ß√µes vistas anteriormente.</div></p>

## Dist√¢ncia do √¢ngulo

<p><div align="justify">Imagine que $\mathcal{A}= S^{\,1} =\{ (a,b)\in\mathbb{R}^2 : a^2 + b^2 = 1 \}$, ou seja, estamos olhando exatamente para os vetores do plano de norma L2 igual a 1, o c√≠rculo tradicional que conhecemos. Podemos definir uma <b>dist√¢ncia do √¢ngulo</b> dada pelo √¢ngulo entre dois pontos. Por exemplo, a dist√¢ncia entre $(0,1)$ e $(1,0)$ seria $\pi/2$, uma vez que o √¢ngulo entre estes dois vetores √© $90^\circ$. Uma forma expl√≠cita de calcular esse √¢ngulo √©</div></p>

$$
\begin{equation*}
d_{\textrm{ang}}((x_1,x_2),(y_1,y_2)) = \arccos \left( x_1\,y_1 + x_2 \,y_2\right),
\end{equation*}
$$

<p><div align="justify">em que o argumento do arco cosseno √© justamente o produto interno entre os vetores. </div></p>

<p><div align="justify">√â importante reparar que fizemos a restri√ß√£o de olhar apenas para vetores de tamanho $1$ pra satisfazer a defini√ß√£o de m√©trica que fizemos. Olhar o √¢ngulo entre vetores de tamanho qualquer n√£o nos d√° uma m√©trica pois n√£o temos a propriedade de identidade: o √¢ngulo entre os vetores $(1,0)$ e $(2,0)$ √© $0$ entretanto $(1,0)\neq (2,0)$. Al√©m disso, n√£o √© poss√≠vel definir o √¢ngulo entre um vetor qualquer e o vetor nulo.</div></p>

<p><div align="justify">Podemos generalizar essa dist√¢ncia para vetores em dimens√µes maiores na superf√≠cie de uma hiper-esfera unit√°ria $\mathcal{A} = S^{\,n-1} = \{ \textbf{x}\in\mathbb{R}^n : ||\textbf{x}||_2 = 1 \}$. Calculamos a  dist√¢ncia entre $\textbf{x} = (x_1, x_2, \cdots, x_n)$ e $\textbf{y} = (y_1, y_2, \cdots, y_n)$, como</div></p>

$$
\begin{equation*}
d_{\textrm{ang}}(\textbf{x},\textbf{y}) = \arccos \left( \sum_{i=1}^n x_i\,y_i\right).
\end{equation*}
$$

<p><div align="justify">Esta forma de definir dist√¢ncia satisfaz as propriedades desejadas, mas n√£o √© f√°cil ver porque a desigualdade triangular √© realizada fora de $S^1$ <i>($\oint$ caso conhe√ßa um pouco de geometria diferencial, a ideia √© que esta defini√ß√£o √© a m√©trica geod√©sica na hiper-esfera unit√°ria)</i>.</div></p>

### Dist√¢ncia entre documentos

<p><div align="justify">Essa m√©trica √© tradicionalmente usada em discuss√µes iniciais sobre dist√¢ncia entre dois textos. Primeiro, temos que pensar em uma representa√ß√£o num√©rica para um texto. Uma maneira inicial √© pensar no texto como uma <i>bag of words</i>, desprezando a ordem das palavras, letras mai√∫sculas e pontua√ß√µes, mas levando em conta a frequ√™ncia de cada palavra no texto. </div></p>

<p><div align="justify">Neste caso, $\mathcal{A} = \mathbb{N}^{t}$, em que $t$ √© o n√∫mero total de palavras diferentes que aparecem no <i>corpus</i> (conjunto de todos os documentos) que desejamos calcular as dist√¢ncias. Cada componente est√° associada com uma dessas palavras. Um texto √© um vetor de $\mathcal{A}$ em que cada elemento nos d√° quantas vezes aquela palavra ocorre no texto.</div></p>

<p><div align="justify">Fica mais f√°cil ver isso com um exemplo: Suponha que nosso <i>corpus</i> √© dado pelos textos $\{$ $\textrm{texto}_1 = $ <i>"Ol√°, bom dia, bom dia."</i>,  $\textrm{texto}_2 = $ <i>"Bom dia!"</i>$\}$. Neste caso, temos o mapa: $\{1\to$ <i>ola</i>, $2\to$ <i>bom</i>, $3\to$ <i>dia</i>$\}$ indicando cada componente do vetor de $\mathbb{N}^{\,3}$ (repare que aqui ignoramos pontua√ß√£o, letras mai√∫sculas e acentos). Ficamos com</div></p>

$$
\begin{equation*}
\{ \textrm{texto}_1 = (1,2,2),  \textrm{texto}_2 = (0,1,1)\}
\end{equation*}
$$

<p><div align="justify">Com esta representa√ß√£o num√©rica, a princ√≠pio, podemos usar qualquer m√©trica vista anteriormente. Uma motiva√ß√£o para usar a dist√¢ncia do √¢ngulo √© assumir que textos parecidos usam as mesmas palavras com uma frequ√™ncia parecida de vezes. Portanto, apontam na mesma dire√ß√£o da nossa representa√ß√£o.</div></p>

<p><div align="justify">A ideia agora √© normalizar os vetores e estamos prontos para calcular a dist√¢ncia. A dist√¢ncia, neste caso √© dada por</div></p>

$$
\begin{equation*}
d_{\textrm{ang}}(\textrm{texto}_1, \textrm{texto}_2) = \arccos \left( \frac{1\cdot0 + 2\cdot1 + 2\cdot 1}{\sqrt{5} \,\cdot \sqrt{2} }  \right) = 0.369\pi.
\end{equation*}
$$

<p><div align="justify">Considerando que essa dist√¢ncia vale no m√≠nimo 0 e no m√°ximo $\pi$, temos textos razoavelmente parecidos.</div></p>

<p><div align="justify">$\oint$ <i>Essa abordagem apresenta in√∫meras simplifica√ß√µes que fazem a gente perder a qualidade da resposta. Por exemplo, sabemos que palavras diferentes podem significar a mesma coisa (ou conjuga√ß√µes diferentes, plurais etc). Palavras iguais em contextos diferentes podem ter significados diferentes. Existem palavras comuns a v√°rios textos ou que ocorrem muitas vezes no mesmo texto que podem n√£o ser √∫teis. Em muitos casos a ordem das palavras √© muito importante e mudam o sentido de uma frase (como usar a palavra "n√£o"). Entre outros problemas.</i></div></p>

## Dist√¢ncia entre fun√ß√µes cont√≠nuas

<p><div align="justify">Seja agora $\mathcal{A}=C^{\,0}[a,b] = \{f \in \mathbb{R}^{[a,b]}: f \textrm{ cont√≠nua}\}$, o conjunto das fun√ß√µes cont√≠nuas com dom√≠nio $[a,b]\subset\mathbb{R}$ e contradom√≠nio $\mathbb{R}$.  Podemos definir a dist√¢ncia entre as fun√ß√µes $f,g\in\mathcal{A}$ como </div></p>

$$
\begin{equation*}
d_{\max}(f,g) = \max_{x \,\in \,[a,b]} |f(x) - g(x)|.
\end{equation*}
$$

<p><div align="justify">Ou seja, a dist√¢ncia entre duas fun√ß√µes √© dada pelo m√°ximo do m√≥dulo da diferen√ßa em cada ponto do intervalo $[a,b]$.</div></p>

<p><div align="justify">Por exemplo, se queremos calcular a dist√¢ncia entre as fun√ß√µes $f$ e $g$ definidas no intervalo $[0,1]$ tais que $f(x)=(x-0.4)^2$ e $g(x) = 2.5$, temos que achar o valor que maximiza a fun√ß√£o $h(x) = | (x-0.4)^2 - 2.5|$, plotada na primeira imagem da Figura 3. Isso nem sempre √© uma tarefa f√°cil, pois n√£o temos nenhuma hip√≥tese sobre a diferenciabilidade das nossas fun√ß√µes e o m√≥dulo atrapalha ainda mais criando novos picos. Na segunda imagem da Figura 2 temos uma interpreta√ß√£o visual do que queremos. O valor da dist√¢ncia ser√° o local em que as curvas est√£o mais distantes.</div></p>

<p><center><img src="{{ site.baseurl }}/assets/img/distancia/imagem3.jpg"></center>
<center><b>Figura 3</b>: Na primeira imagem temos o m√≥dulo da diferen√ßa das fun√ß√µes avaliadas que chamamos de $h$. Na segunda imagem temos $f$ em vermelho, $g$ em azul e alguns valores da dist√¢ncia pontual das fun√ß√µes em alguns valores de $x$ em cinza. Em preto temos o ponto que representa a dist√¢ncia entre essas duas fun√ß√µes, valendo $2.5$, nesse caso.</center></p>

<p><div align="justify">Nessa m√©trica, a bola de raio $r>0$ ao redor da fun√ß√£o $f:[a,b]\to\mathbb{R}$ s√£o todas as fun√ß√µes (definidas no intervalo $[a,b]$) que ficam sempre dentro da faixa ao redor de $f$ de largura $2r$. Na Figura 4 temos um exemplo disso. A fun√ß√£o $g(x) = (x-0.4)^2 + 0.4\sin(30x)$ est√° dentro da bola $B_{d_{\max}}((x-0.4)^2;0.5)$ pois a a dist√¢ncia entre elas √© $\max_{x\in[0,1]} |0.4 \sin(30x) |=0.4<0.5$.</div></p>

<p><center><img src="{{ site.baseurl }}/assets/img/distancia/imagem4.jpg"></center>
<center><b>Figura 4</b>: A bola de raio $0.5$ centrada na fun√ß√£o preta s√£o todas as fun√ß√µes que est√£o limitadas pela faixa vermelha. A fun√ß√£o verde √© um exemplo que est√° na bola pois n√£o sai desse limite.</center></p>

<p><div align="justify">$\oint$ <i>O teste KS usa uma varia√ß√£o dessa m√©trica pra definir uma dist√¢ncia entre vari√°veis aleat√≥rias a partir da dist√¢ncia entre as fun√ß√µes densidade acumulada.</i></div></p>

## Dist√¢ncia Ponderada

<p><div align="justify">Em muitos casos pode ser importante atribuir um peso maior para alguma das coordenadas, surgindo a ideia da <b>dist√¢ncia ponderada</b>. Por exemplo, se $\mathcal{A}=\mathbb{R}^2$ e estar perto na primeira coordenada √© $10$ vezes mais importante do que estar perto na , segunda, podemos fazer uma varia√ß√£o da m√©trica Euclidiana para calcular a dist√¢ncia entre $\textbf{x} = (x_1,x_2)$ e $\textbf{y}=(y_1,y_2)$ como</div></p>

$$
\begin{equation*}
 d_{\textrm{ponderada}}(\textbf{x},\textbf{y}) = \sqrt{10 (x_1-y_1)^2 +(x_2-y_2)^2 }\,.
\end{equation*}
$$

<p><div align="justify">N√£o entraremos em detalhes, mas podemos fazer isso sempre que temos uma matriz $A$ <a href="https://pt.wikipedia.org/wiki/Matriz_positiva_definida">matriz positiva definida</a> definindo</div></p>

$$
\begin{equation*}
 d_{\textrm{ponderada}}(\textbf{x},\textbf{y}) = \sqrt{(\textbf{x}-\textbf{y})^T \, A \,(\textbf{x}-\textbf{y}) }\,,
\end{equation*}
$$

<p><div align="justify">fazendo as opera√ß√µes usuais de multiplica√ß√£o de vetores linha, coluna e matrizes. Portanto, fixada $A$, podemos implementar essa m√©trica para usar a fun√ß√£o <code>bola_aberta</code> como fizemos com a m√©trica discreta.</div></p>

```python
def ponderada(X, Y):
    """
    X, Y: vetores que queremos calcular a dist√¢ncia ponderada (np array of floats)
    return dist√¢ncia ponderada entre o ponto X e Y
    """
    return np.dot(X-Y,np.matmul(A,X-Y))
```

<p><div align="justify">Uma matriz com todos valores da diagonal positivos √© sempre uma matriz positiva definida. Neste caso podemos interpretar os valores da diagonal como os pesos que queremos dar em cada uma das coordenadas. A dist√¢ncia euclidiana usual ocorre quando $A$ √© a matriz identidade. J√° o caso estudado anteriormente ocorre quando</div></p>

<p><center>$A = \left[\begin{array}{cc} 10 & 0 \\ 0 & 1 \end{array}\right]$</center></p>

<p><div align="justify">Podemos brincar com essas diferentes matrizes colocando pesos nas coordenadas que consideramos mais importantes. Valores fora da diagonal principal podem ser interpretados como uma intera√ß√£o entre aquelas coordenas. Eles v√£o distorcer o formato da bola, como podemos ver na Figura 5 em que temos, respectivamente, as matrizes positivas definidas</div></p>

<p><center>$A= \left[\begin{array}{cc} 2 & 0 \\ 0 & 1 \end{array}\right], \left[\begin{array}{cc} 2 & -1 \\ -1 & 2 \end{array}\right]\textrm{, e }\left[\begin{array}{cc} 1 & -1 \\ -1 & 4 \end{array}\right].$</center></p>

<p><center><img src="{{ site.baseurl }}/assets/img/distancia/imagem5.jpg"></center>
<center><b>Figura 5</b>: Bolas de raio 1 e centro na origem para as matrizes apresentadas anteriormente, respectivamente.</center></p>

___

# Explicabilidade por refer√™ncia

<p><div align="justify">A no√ß√£o de dist√¢ncia √© um conceito muito importante. Ela √© um jeito intuitivo de nos dar uma no√ß√£o de <b>similaridade</b> (e <b>dissimilaridade</b>) entre exemplos. No contexto de aprendizado supervisionado, se sua forma de calcular dist√¢ncia √© razoavelmente compreensica, modelos que funcionam atrav√©s de encontrar vizinhos pr√≥ximos tornam-se <b>explic√°veis por refer√™ncia</b>. Se queremos entender o motivo dele ter dado determinada previs√£o para um exemplo, olhamos para os dados do treino parecidos com a observa√ß√£o que estamos avaliando.</div></p>

<p><div align="justify">Colocando em um exemplo, suponha que temos um problema de classifica√ß√£o bin√°ria em que estamos tentando prever se um cliente √© inadimplente ou n√£o. Usar estrat√©gias que encontram vizinhos pr√≥ximos na base de treinamento nos d√° explica√ß√µes para a resposta de um determinado cliente. Estamos olhando clientes com atributos parecidos (segundo nossa dist√¢ncia) e fazendo a previs√£o a partir deles. Se queremos saber porque um determinado cliente foi considerado inadimplente, olhamos para os vizinhos mais pr√≥ximos e entendemos porque o modelo deu o resultado que deu.</div></p>

___

<p><div align="justify">Espero que este post, muito divertido de escrever, tenha sido √∫til para entender melhor o que significa dist√¢ncia e as diferen√ßas entre elas. No m√≠nimo, agora voc√™ pode usar algumas das m√©tricas que discutimos aqui como um dos hiper-par√¢metro dos seus modelos baseados em dist√¢ncia! </div></p>

<p><div align="justify"><i>$\oint$ Nem sempre a troca √© imediata. No k-means, por exemplo, a atualiza√ß√£o dos centroides √© a m√©dia dos exemplos daquele cluster justamente porque a m√©dia minimiza a fun√ß√£o custo dada pela soma das dist√¢ncias euclidianas dos exemplos at√© os seus respectivos centroides. Se escolhemos minimizar a soma das dist√¢ncias de Minkowski de par√¢metro 1 (L1), ent√£o a atualiza√ß√£o dos centroides √© feita com a mediana coordenada a coordenada uma vez que estes s√£o os valores que minizam a fun√ß√£o de custo alterada, este √© o K-Medians.</i></div></p>