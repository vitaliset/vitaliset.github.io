---
layout: post
title: Uma utilização crítica do Boruta
featured-img: boruta
category: [feature selection]
mathjax: true
summary: Um passo a passo da utilização do Boruta, discutindo a motivação da construção e debatendo variações úteis do algoritmo.
---

<p><div align="justify">Se fixarmos o poder preditivo no conjunto de desenvolvimento, um modelo com menos atributos tende a ter menor propensão de abusar de ruídos e relações espúrias do seu conjunto de treinamento, o que pode levá-lo a ganhos de performance fora do laboratório. Uma seleção bem feita de variáveis é, portanto, uma ferramenta *data-centric* importante na modelagem de problemas de aprendizado de máquina supervisionado.</div></p>

<p><div align="justify"><i>$\oint$ Para ilustrar a afirmação anterior, temos, como exemplo, que a <a href="https://youtu.be/Dc0sr0kdBVI">dimensão-VC</a> (medida de complexidade de uma família de hipóteses) de um perceptron (classificador linear) é $d+1$, em que $d$ é o número de variáveis utilizadas no modelo [<a href="#bibliography">1</a>]. Um modelo com dimensão-VC maior significa que você precisa de um volume de dados maior para garantir que sua performance, medida no treinamento, seja semelhante à performance real. Na prática, isso significa que quanto maior a dimensão-VC, maior a chance de overfitting. Consequentemente, nesse exemplo, se temos dois perceptrons com performances semelhantes no treino, com a diferença de que um tem mais variáveis que o outro, o que tem mais variáveis tem maior chance de apresentar overfitting [<a href="#bibliography">1</a>].</i></div></p>

<p><div align="justify">Entretanto, a seleção de variáveis não é vista com o cuidado devido na maioria dos cursos de Aprendizado de Máquina. São apresentados poucos métodos e de maneira superficial. Os poucos lugares que discutem o tema, no geral, focam ainda em técnicas que são pouco escaláveis com o aumento de variáveis e, por isso, são pouco praticáveis na maioria das aplicações do mercado (como as estratégias gulosas de <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html"><code>sklearn.feature_selection.SequentialFeatureSelector</code></a>).</div></p>

<p><div align="justify">No <a href="https://br.linkedin.com/showcase/serasa-experian-datalab">DataLab da Serasa Experian</a>, seleção de variáveis se torna extremamente relevante pela natureza dos problemas que trabalhamos. Na grande maioria dos casos temos algumas milhares de variáveis disponíveis no bureau de dados da Serasa e não é fácil identificar de antemão quais serão as features que nos darão mais ganhos. É necessário aplicar técnicas que são robustas à grandeza do número de variáveis que temos ao mesmo tempo que garantam uma seleção que faça sentido.</div></p>

<p><div align="justify">Neste post, iremos motivar a construção do Boruta [<a href="#bibliography">2</a>], uma das técnicas mais utilizadas pelos cientistas do <a href="https://br.linkedin.com/showcase/serasa-experian-datalab">DataLab</a> na seleção de features, com algumas dicas de uso prático. Ilustraremos ainda o uso da função <a href="https://github.com/scikit-learn-contrib/boruta_py"><code>boruta.BorutaPy</code></a>, do ambiente <a href="https://github.com/scikit-learn-contrib/scikit-learn-contrib/blob/master/README.md">scikit-learn-contrib</a> (ou seja, compátivel com bibliotecas que sigam o <a href="https://scikit-learn.org/stable/developers/develop.html">padrão de código do scikit-learn</a>).</div></p>

___

<p><div align="justify">Para ilustrar o problema de seleção de features, utilizaremos o <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html"><code>sklearn.datasets.make_classification</code></a> para criar um problema genérico de classificação em que podemos definir, como um parâmetro da função, o número de variáveis úteis para o problema de previsão.</div></p>

```python
from sklearn.datasets import make_classification

N_FEATURES = 20

X, y = \
make_classification(n_samples=1000,
                    n_features=N_FEATURES,
                    n_informative=2,
                    n_redundant=2,
                    n_classes=2,
                    flip_y=0.1,
                    shuffle=False,
                    random_state=42)

X = pd.DataFrame(X, columns=[f'column_{i+1}' for i in range(N_FEATURES)])

X.head()
```

![png](Sele%C3%A7%C3%A3o%20de%20vari%C3%A1veis%3A%20uma%20utiliza%C3%A7%C3%A3o%20cr%C3%ADtica%20do%20Boruta_files/output_2_0.png)

<p><div align="justify">Como estamos escolhendo 2 features informativas e 2 features redundantes, temos que as 4 features mais importantes são as colunas: <code>column_1</code>, <code>column_2</code>, <code>column_3</code> e <code>column_4</code>.</div></p>

# Motivando a construção do Boruta

## Medindo a importância de uma variável

<p><div align="justify">Uma das técnicas mais comuns para selecionar as variáveis é aproveitar-se de modelos que, de alguma forma, selecionam-nas no próprio processo de treinamento. Árvores e, consequentemente, cômites de árvores são, talvez, o melhor exemplo disso: pela <a href="https://www.edureka.co/community/46109/what-is-greedy-approach-in-decision-tree-algorithm">estratégia gulosa de fazer a melhor quebra possível naquele instante</a> (de acordo com algum critério de melhor, usualmente relacionado à pureza das folhas criadas, no caso de classificação), estamos sempre escolhendo variáveis relevantes. Variáveis pouco discriminativas são utilizadas muito menos que as variáveis que de fato ajudam a fazer a previsão [<a href="#bibliography">3</a>].</div></p>

<p><div align="justify">Esse processo, naturalmente deriva medidas de importância para as variáveis como: o número de vezes que ela é utilizada (esse é o modo default do atributo <code>.feature_importance_</code> dos ensembles do LGBM, como o <a href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html"><code>lightgbm.LGBMClassifier</code></a>) ou uma ponderação do ganho de informação durante a escolha das quebras das features (essa é a forma default dos ensembles de árvores do sklearn, como o <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"><code>sklearn.ensemble.RandomForestClassifier</code></a>, o <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html"><code>sklearn.ensemble.ExtraTreesClassifier</code></a>, e o <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html"><code>sklearn.ensemble.HistGradientBoostingClassifier</code></a>, além de também virar o atributo do LGBM quando definimos o <code>importance_type=&#39;gain&#39;</code>).</div></p>

<p><div align="justify">Com alguma dessas medidas naturais de importância, é razoável ordenar nossas variáveis da mais importante para a menos importante.</div></p>

```python
from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(random_state=42).fit(X, y)

df_imp = \
(pd.DataFrame(list(zip(X.columns, rfc.feature_importances_)),
              columns=['feature_name', 'feature_importance'])
 .sort_values(by='feature_importance', ascending=False)
 .reset_index(drop=True)
)

df_imp
```

![png](Sele%C3%A7%C3%A3o%20de%20vari%C3%A1veis%3A%20uma%20utiliza%C3%A7%C3%A3o%20cr%C3%ADtica%20do%20Boruta_files/output_5_0.png)

<p><div align="justify"><i>$\oint$ Existem algumas outras formas de metrificar a importância de uma variável como, por exemplo, utilizando suas contribuições de <a href="https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30">valores SHAP</a>. Tendo em vista que o <a href="https://github.com/slundberg/shap"><code>shap.Explainer(model).shap_values(X)</code></a> nos retorna uma medida de quanto aquela variável agregou na previsão, pegar a sua média entre todos os exemplos nos dá uma forma de ver o quão útil ela foi para discriminar os exemplos como um todo. Para os valores não se cancelarem (imagine uma variável que para determinados valores joga a previsão para cima e em outros valores joga a previsão para baixo), tomamos o módulo antes de fazer a média. Repare que a ordem das importâncias dada pelo SHAP pode ser diferente da ordem de importâncias dada pelo atributo de <code>.feature_importance_</code> usual do estimador, como é o caso do nosso exemplo.</i></div></p>

```python
explainer = shap.TreeExplainer(rfc)
shap_vals = explainer.shap_values(X)

df_imp_shap = \
(pd.DataFrame(list(zip(X.columns, np.abs(shap_vals[0]).mean(axis=0))),
              columns=['feature_name', 'shap_importance'])
 .sort_values(by='shap_importance', ascending=False)
 .reset_index(drop=True)
)

df_imp_shap
```

![png](Sele%C3%A7%C3%A3o%20de%20vari%C3%A1veis%3A%20uma%20utiliza%C3%A7%C3%A3o%20cr%C3%ADtica%20do%20Boruta_files/output_7_0.png)

<p><div align="justify"><i>Ainda não falamos do Boruta, mas ele se utiliza dessa ordenação para fazer suas análises e é implementado, usualmente, utilizando medida de importância do estimador (o atributo <code>.feature_importances_</code> ou <code>.coef_</code> para algoritmos lineares). Essa diferença motivou alguns contribuidores a implementar o <a href="https://github.com/Ekeany/Boruta-Shap">Boruta-Shap</a>. Entretanto, incorporar o SHAP ao processo do Boruta não parece trivial e a biblioteca costuma ser lenta.</i></div></p>

<p><div align="justify"><i>Uma possível alternativa pode ser adaptar na mão o atributo <code>.feature_importance_</code> do seu classificador, salvando o <code>X</code> no momento de treinamento para utilização no cálculo do SHAP. Como implemento aqui:</i></div></p>

```python
class SHAPImportanceRandomForestClassifier(RandomForestClassifier):
    def fit(self, X, y, sample_weight=None):
        self.X_ = X
        super().fit(X, y, sample_weight=sample_weight)
        return self
    @property
    def feature_importances_(self):
        check_is_fitted(self)
        explainer = shap.TreeExplainer(self)
        shap_vals = explainer.shap_values(self.X_)
        return np.abs(shap_vals[0]).mean(axis=0)
```

```python
from shap_feature_importances_ import SHAPImportanceRandomForestClassifier

rfc_shap = SHAPImportanceRandomForestClassifier(random_state=42).fit(X, y)
rfc_shap.feature_importances_
```

    array([0.04156985, 0.19764501, 0.10721142, 0.04379691, 0.00509938,
           0.00967927, 0.00900892, 0.00769202, 0.01053711, 0.00973848,
           0.00764462, 0.00725161, 0.00690175, 0.00718789, 0.00600269,
           0.00526766, 0.00659648, 0.00585107, 0.00726538, 0.00501896])

<p><div align="justify"><i>Note que essa implementação utiliza o mesmo conjunto de treino para cálculo do SHAP. Existe algum debate aqui, mas tenha em mente que os valores de importância calculados com SHAP (média do valor absoluto) no teste podem ser diferentes dos valores de importância calculados com SHAP no treino. Se você quiser esse nível de preciosismo, pode estar interessado em reservar um pedaço do seu conjunto de dados para calcular os valores SHAP. Implemento essa ideia na classe <a href="https://github.com/vitaliset/blog-notebooks/blob/main/DataLab_Blog_Boruta_2022_09_05/shap_feature_importances_.py"><code>XSHAPImportanceRandomForestClassifier</code></a> do arquivo <a href="https://github.com/vitaliset/blog-notebooks/blob/main/DataLab_Blog_Boruta_2022_09_05/shap_feature_importances_.py"><code>shap_feature_importances_.py</code></a> no <a href="https://github.com/vitaliset/blog-notebooks/tree/main/DataLab_Blog_Boruta_2022_09_05">repositório deste post</a>. Entretanto, para poder dormir tranquilo, tenha em mente que o <code>.feature_importances_</code> usual dos algoritmos baseados em árvore é calculado com o conjunto de treino, então calcular o SHAP no treino não é uma blasfêmia tão grande.</i></div></p>

## Selecionando as `K` "melhores variáveis"

<p><div align="justify">Se quisermos que nosso modelo tenha apenas as <code>K</code> features mais úteis, a maneira natural de escolher elas seria pegar as <code>K</code> variáveis com maiores valores de importância.</div></p>

```python
K = 4

(df_imp
 .head(K)
 .feature_name
 .to_list()
)
```

    ['column_2', 'column_3', 'column_4', 'column_1']

<p><div align="justify">Essa é uma das estratégias mais comuns de se fazer seleção de features no mercado, mas levanta algumas questões. A primeira e mais imediata é: como escolher o número de variáveis <code>K</code> ideal. Nesse caso ilustrativo, sabemos que 4 variáveis é o número correto, mas na maioria dos casos de aplicação real é irrealista ter esse número de antemão.</div></p>

<p><div align="justify"><i>$\oint$ Uma estratégia muito utilizada, mas que não vamos focar muito, é aumentar a lista de features do modelo seguindo a ordenação dada pelo modelo treinado em todas as features, encarando esse valor <code>K</code> como um hiper-parâmetro que estamos otimizando. No exemplo abaixo, fazemos isso utilizando o <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"><code>sklearn.model_selection.GridSearchCV</code></a> ao construir uma classe <a href="https://github.com/vitaliset/blog-notebooks/blob/main/DataLab_Blog_Boruta_2022_09_05/selectktop_selector.py"><code>SelectKTop</code></a> utilizando o padrão necessário para os selecionadores de variáveis do scikit-learn, isto é, seguindo a forma que o <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectorMixin.html"><code>sklearn.feature_selection.SelectorMixin</code></a> exige. Você pode ver a implementação dessa classe no arquivo <a href="https://github.com/vitaliset/blog-notebooks/blob/main/DataLab_Blog_Boruta_2022_09_05/selectktop_selector.py"><code>selectktop_selector.py</code></a> no <a href="https://github.com/vitaliset/blog-notebooks/tree/main/DataLab_Blog_Boruta_2022_09_05">repositório deste post</a>.</i></div></p>

<p><div align="justify"><i>PS: A classe <a href="https://github.com/vitaliset/blog-notebooks/blob/main/DataLab_Blog_Boruta_2022_09_05/selectktop_selector.py"><code>SelectKTop</code></a> é mais ou menos equivalente à classe <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html"><code>sklearn.feature_selection.SelectFromModel</code></a>, cuja existência descobri após terminar de escrever o post!</i></div></p>

```python
from selectktop_selector import SelectKTop

from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold
from sklearn.pipeline import make_pipeline

grid = (
    GridSearchCV(
        make_pipeline(SelectKTop(random_state=42),
                      RandomForestClassifier(random_state=42)),
        param_grid={'selectktop__K': np.arange(1,N_FEATURES+1)},
        cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42),
        scoring='roc_auc')
    .fit(X, y))

df_cv = (
    pd.DataFrame(grid.cv_results_)[[
        'param_selectktop__K',
        'mean_test_score',
        'std_test_score'
    ]])

cv_best = (
    df_cv
    .sort_values(by='mean_test_score', ascending=False)
    .reset_index(drop=True)
    .loc[0])
```

```python
plt.errorbar(df_cv.param_selectktop__K, df_cv.mean_test_score, 1.96*df_cv.std_test_score)
plt.scatter(cv_best.param_selectktop__K, cv_best.mean_test_score, s=100)
plt.ylim(0.75, 1)
plt.xlabel('K of SelectKTop')
plt.xticks(df_cv.param_selectktop__K.astype(int))
plt.ylabel('Performance (ROCAUC)')
plt.show()
```

![png](Sele%C3%A7%C3%A3o%20de%20vari%C3%A1veis%3A%20uma%20utiliza%C3%A7%C3%A3o%20cr%C3%ADtica%20do%20Boruta_files/output_15_0.png)

<p><div align="justify"><i>No nosso experimento controlado, encontramos algumas poucas variáveis a mais do que o correto (e ficamos com todas as úteis).</i></div></p>

```python
grid.best_estimator_.steps[0][1].get_feature_names_out()
```

    array(['column_1', 'column_2', 'column_3', 'column_4', 'column_10'],
          dtype=object)

<p><div align="justify"><i>Vale citar que podemos deixar esse método mais robusto variando o <code>random_state</code> do <code>base_estimator</code> e tendo uma distribuição de importâncias para cada variável ao invés de apenas um valor único (que naturalmente é mais ruidoso). Utilizar essa técnica com o SHAP para medir a importância (passando por exemplo o <a href="https://github.com/vitaliset/blog-notebooks/blob/main/DataLab_Blog_Boruta_2022_09_05/shap_feature_importances_.py"><code>SHAPImportanceRandomForestClassifier</code></a> como <code>base_estimator</code> do <a href="https://github.com/vitaliset/blog-notebooks/blob/main/DataLab_Blog_Boruta_2022_09_05/selectktop_selector.py"><code>SelectKTop</code></a>) é algo muito utilizado por alguns cientistas do <a href="https://br.linkedin.com/showcase/serasa-experian-datalab">DataLab</a> como alternativa ao Boruta que, como vamos ver, costuma ser muito demorado.</i></div></p>

## Selecionando as K melhores variáveis com ponto de corte sugerido por uma variável aleatória

<p><div align="justify">Criar uma variável de ruído, ou seja, que sabidamente não é útil para a previsão, nos auxilia a ter um ponto de corte para filtro das variáveis que demonstram ajudar na previsão. A ideia dessa abordagem é medir a importância da variável aleatória e ficar apenas com variáveis que se demonstrarem mais importantes do que ela.</div></p>

<p><div align="justify">Adicionando a nova coluna, por exemplo, amostrada de uma variável aleatória $\mathcal{N}(0,1)$ de forma independente, temos uma nova lista de importância das variáveis.</div></p>

```python
normal_noise_X = (X.assign(noise_column = np.random.RandomState(42).normal(size=X.shape[0])))
normal_noise_X[normal_noise_X.columns[::-1]].head()
```

![png](Sele%C3%A7%C3%A3o%20de%20vari%C3%A1veis%3A%20uma%20utiliza%C3%A7%C3%A3o%20cr%C3%ADtica%20do%20Boruta_files/output_20_0.png)

```python
normal_noise_rfc = RandomForestClassifier(random_state=42).fit(normal_noise_X, y)

df_imp_normal_noise = \
(pd.DataFrame(list(zip(normal_noise_X.columns, normal_noise_rfc.feature_importances_)),
              columns=['feature_name', 'feature_importance'])
 .sort_values(by='feature_importance', ascending=False)
)

df_imp_normal_noise
```

![png](Sele%C3%A7%C3%A3o%20de%20vari%C3%A1veis%3A%20uma%20utiliza%C3%A7%C3%A3o%20cr%C3%ADtica%20do%20Boruta_files/output_21_0.png)

<p><div align="justify">Como a última variável é a nossa coluna sabidamente ruidosa, a ideia dessa técnica é selecionar apenas as variáveis que têm importância maior do que o limiar definido pela importância da variável não relacionada.</div></p>

```python
normal_noise_importance = \
normal_noise_rfc.feature_importances_[-1]

np.array(
 df_imp_normal_noise
 .query(f"feature_importance > {normal_noise_importance}")
 .feature_name
)
```

    array(['column_2', 'column_3', 'column_4', 'column_1', 'column_6',
           'column_10', 'column_14'], dtype=object)

<p><div align="justify">Vale observar que, a escolha da variável ruidosa como $\mathcal{N}(0,1)$ foi totalmente arbitrária. Entretanto, isso faz diferença e pode fazer com que a seleção de variáveis seja distinta. No nosso exemplo controlado, mudar o ruído para $\textrm{Exp}(1)$ nos faria selecionar variáveis finais diferentes totalmente por sorte.</div></p>

```python
exp_noise_X = \
(X.assign(noise_column = np.random.RandomState(42).exponential(size=X.shape[0])))
exp_noise_rfc = \
RandomForestClassifier(random_state=0).fit(exp_noise_X, y)
exp_noise_importance = \
exp_noise_rfc.feature_importances_[-1]

np.array(
 pd.DataFrame(list(zip(exp_noise_X.columns, exp_noise_rfc.feature_importances_)),
              columns=['feature_name', 'feature_importance'])
 .sort_values(by='feature_importance', ascending=False)
 .query(f"feature_importance > {exp_noise_importance}")
 .feature_name
)
```

    array(['column_2', 'column_3', 'column_4', 'column_1', 'column_14',
           'column_6', 'column_10', 'column_9', 'column_12', 'column_13',
           'column_7', 'column_18'], dtype=object)

<p><div align="justify">Isso nos demonstra um problema desse método. Apesar de poderoso, por nos dar um jeito interessante de selecionar as variáveis sem escolher <code>K</code> de forma arbitrária, a escolha da distribuição da variável ruidosa é uma fonte de variação relevante.</div></p>

<p><div align="justify">Em muitos casos, ter variáveis discretas versus contínuas pode influenciar na medida de importância (como é o caso de árvores que, por terem mais quebras disponíveis, terão mais chance de escolher uma variável ruidosa contínua) ou, ainda, a própria escala da feature adicionada pode atrapalhar nessa mensuração (por exemplo, se estamos usando os coeficientes angulares de um <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html"><code>sklearn.linear_model.Lasso</code></a>).</div></p>

<p><div align="justify">Toda essa variabilidade pode fazer com que, as vezes, uma feature ruim seja selecionada, ao passo que uma variável boa seja descartada por azar.</div></p>

<p><div align="justify">O Boruta vem para tentar lidar com essas duas questões ao mesmo tempo: tentar manter a distribuição marginal das features ruidosas iguais às distribuições marginais das features originais, enquanto tenta ser robusto à variabilidade, repetindo o experimento algumas vezes.</div></p>

# Ideias gerais do Boruta

<p><div align="justify">Já existem muitos textos úteis que explicam o Boruta de forma didática e com exemplos. Como a ideia desse post não é ser redundante com a literatura e sim compilar ideias centrais de uso prático, vamos apenas citar os principais aspectos e deixar o convite para uma leitura detalhada de outras referências do tema como o post <a href="https://towardsdatascience.com/boruta-explained-the-way-i-wish-someone-explained-it-to-me-4489d70e154a">Boruta Explained Exactly How You Wished Someone Explained to You</a>. A construção que fizemos anteriormente vai deixar as ideias do Boruta ainda mais claras, justificando o seu modo de ser.</div></p>

<p><div align="justify">Em resumo, o Boruta [<a href="#bibliography">2,4</a>]:</div></p>
- <p><div align="justify">Cria variáveis não correlacionadas com a <em>target</em> ao embaralhar, entre as linhas, variáveis já presentes no dataset (essas são as variáveis que chamamos de <em>shadow</em>).</div></p>
- <p><div align="justify">Lida com a variabilidade repetindo o processo várias vezes e marcando quantas vezes a nossa variável de interesse ficou atrás do percentil <code>perc</code> dos <code>.feature_importances</code> das <em>shadow features</em> (por default <code>perc=100</code>, portanto, comparamos com o máximo dos <code>.feature_importances</code> das <em>shadow features</em>, isto é, se alguma <em>shadow</em> for melhor, já descartamos aquela variável de interesse naquela rodada).</div></p>
- <p><div align="justify">Por fim, um teste de hipótese é feito para avaliar se podemos afirmar com alguma significância estatística <code>alpha</code> que a feature de interesse é melhor que o percentil <code>perc</code> da importância das <em>shadow features</em>.</div></p>
- <p><div align="justify">O teste de hipótese divide o conjunto de features em três categorias:</div></p>
    - <p><div align="justify">As variáveis que estatisticamente são variáveis melhores que as <em>shadow features</em> (são as chamadas de <code>.support_</code>);</div></p>
    - <p><div align="justify">As variáveis que estatisticamente são equivalentes às variáveis <em>shadow</em> (variáveis que excluímos);</div></p>
    - <p><div align="justify">As variáveis que não são possíveis de afirmar com significância estatística como sendo melhores que as variáveis <em>shadow</em> (<code>.weak_support_</code>).</div></p>
- <p><div align="justify">Na prática, a partir do momento que ele tem confiança de que uma determinada variável não é importante, ele já a exclui das próximas iterações.</div></p>


# O [boruta.BorutaPy](https://github.com/scikit-learn-contrib/boruta_py)

<p><div align="justify">Primeiro, precisamos instanciar um <code>base_estimator</code> que será utilizado dentro do <a href="https://github.com/scikit-learn-contrib/boruta_py"><code>boruta.BorutaPy</code></a> para calcular a importância das variáveis (através do <code>.feature_importances_</code> ou do <code>.coef_</code>). É importante ressaltar que podemos adicionar hiper-parâmetros que acharmos relevantes para o problema, como o <code>class_weight</code> se temos um problema muito desbalanceado.</div></p>

<p><div align="justify">Quando usamos um cômite de árvores, é importante ter em mente que árvores profundas vão mudar o <code>.feature_importances_</code>, mas vão demorar mais para treinar. É justificável utilizar árvores mais rasas, uma vez que os ganhos mais expressivos são feitos nas primeiras quebras, usualmente.</div></p>

<p><div align="justify">O <a href="https://github.com/scikit-learn-contrib/boruta_py"><code>boruta.BorutaPy</code></a> aceita qualquer estimador que tenha o atributo <code>.feature_importances_</code> disponível após rodar o método <code>.fit(X, y)</code> [<a href="#bibliography">5</a>]. Você pode utilizar isso a seu favor usando os estimadores mais adequados para o seu problema, inclusive, utilizando algoritmos baseados em árvores mais eficientes como as <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html"><code>sklearn.ensemble.ExtraTreesClassifier</code></a> (tenha em mente que as Extra Randomized Trees vão ter seu <code>.feature_importances_</code> afetado pelo método de construção e isso pode impactar a escolha final de variáveis).</div></p>

<p><div align="justify">Para exemplificar a utilização prática da biblioteca, vou utilizar o <a href="https://github.com/vitaliset/blog-notebooks/blob/main/DataLab_Blog_Boruta_2022_09_05/shap_feature_importances_.py"><code>SHAPImportanceRandomForestClassifier</code></a> que criamos anteriormente (basicamente um <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"><code>sklearn.ensemble.RandomForestClassifier</code></a> com SHAP no lugar do <code>.feature_importances_</code> usual).</div></p>


```python
from boruta import BorutaPy

boruta_forest = SHAPImportanceRandomForestClassifier(max_depth=7, random_state=42)
```

<p><div align="justify">Um ponto de atenção que não é necessariamente claro na documentação, é que o parâmetro <code>n_estimators</code> do <a href="https://github.com/scikit-learn-contrib/boruta_py"><code>boruta.BorutaPy</code></a> sobrescreve o <code>n_estimators</code> do estimador como podemos ver no <a href="https://github.com/scikit-learn-contrib/boruta_py/blob/3cf4de864e83ad0c50e0cfa177b2bc2aa4735256/boruta/boruta_py.py#L268">código fonte do BorutaPy</a>:</div></p>

```python
# set n_estimators
if self.n_estimators != 'auto':
    self.estimator.set_params(n_estimators=self.n_estimators)
```
<p><div align="justify">Por default, temos <code>n_estimators=1000</code>. Se <code>n_estimators=&#39;auto&#39;</code>, então <a href="https://github.com/scikit-learn-contrib/boruta_py/blob/3cf4de864e83ad0c50e0cfa177b2bc2aa4735256/boruta/boruta_py.py#L371">uma regra baseada no número de features que estamos avaliando é feita para escolher o número de árvores do ensemble</a>.</div></p>

<p><div align="justify">Por fim, <code>alpha</code> e <code>perc</code> são os outros parâmetros importantes do <a href="https://github.com/scikit-learn-contrib/boruta_py"><code>boruta.BorutaPy</code></a> que você deveria ficar atento:</div></p>
- <p><div align="justify">O <code>perc</code> (percentil do <code>.feature_importances_</code> das <em>shadow features</em> utilizado para decidir se as variáveis foram boas ou não naquela determinada rodada) é um <code>int</code> que vai de 0 a 100. Quanto mais próximo de 100, mais rigoroso estamos sendo na hora de avaliar nossas features. Pela aleatoriedade, alguns <code>.feature_importances_</code> de <em>shadow features</em> podem ser grandes e muito rigorosos com o critério de corte, nesse caso, isso será ruim porque estaremos excluindo variáveis marginais que são relevantes, mas não têm uma importância tão expressiva. O default desse parâmetro é 100, mas recomendo abaixá-lo levemente (para 90, por exemplo) caso esteja trabalhando com um problema com muitas variáveis, desse modo haverá maior chance de se ter uma <em>shadow feature</em> com importância alta.</div></p>
- <p><div align="justify">O <code>alpha</code> é um float que vai de 0 a 1 e é importante para delimitar a partição que fazemos do conjunto de variáveis (<code>.support_weak_</code>, <code>.support</code> e excluídas), uma vez que determinará o rigor de certeza que queremos ter para afirmar que uma determinada feature é relevante ou não para o problema de classificação (ou regressão). O default desse parâmetro é 0.05, e eu não tenho o costume de alterá-lo, pois prefiro mantê-lo fixo e variar o <code>perc</code> já que os dois se relacionam.</div></p>

```python
boruta = \
(BorutaPy(
    estimator=boruta_forest,
    n_estimators=50,
    max_iter=100, # number of trials to perform
    random_state=42)
 .fit(np.array(X), np.array(y)) # fit accepts np.array, not pd.DataFrame
)
```

<p><div align="justify">Por fim, é fácil resgatar as features com os atributos <code>.support_</code> e <code>.support_weak_</code>.</div></p>

```python
green_area = X.columns[boruta.support_].to_list()
blue_area = X.columns[boruta.support_weak_].to_list()

print('Support columns:', green_area)
print('Weak support columns:', blue_area)
```

    Support columns: ['column_1', 'column_2', 'column_3', 'column_4', 'column_10']
    Weak support columns: ['column_9']

# Trade-off de "qualidade da seleção" vs "tempo" quando damos um undersample

<p><div align="justify">Quando temos um dataset muito grande, o <a href="https://github.com/scikit-learn-contrib/boruta_py"><code>boruta.BorutaPy</code></a> pode demorar bastante tempo para rodar pelo processo de criar tantas variáveis <em>shadows</em> quanto temos no conjunto inicial de variáveis. Em muitas aplicações práticas é necessário aplicar o <a href="https://github.com/scikit-learn-contrib/boruta_py"><code>boruta.BorutaPy</code></a> em um subconjunto do seu conjunto de treinamento.</div></p>

<p><div align="justify">Faremos aqui um experimento para ver, em um caso sintético de <code>make_classification</code> com <code>n_samples=5000</code>, <code>n_features=100</code>, <code>n_informative=40</code> e <code>n_redundant=10</code>, como seriam as escolhas de variáveis de um <a href="https://github.com/scikit-learn-contrib/boruta_py"><code>boruta.BorutaPy</code></a> conforme variamos o parâmetro <code>frac</code> de um <a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html"><code>.sample</code></a> feito na base de desenvolvimento.</div></p>

```python
from boruta_sample_experiment import experiment, plot_heatmap, plot_percentage_time

dic_sample, matrix, X_big, y_big = \
experiment(fracs=[0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])
```
    100%|██████████| 11/11 [14:08<00:00, 77.18s/it]

<p><div align="justify">Como o número de variáveis informativas mais o número de variáveis redundantes é 50 então, nesse exemplo controlado, metade das nossas features são importantes. No plot abaixo, para diferentes valores de <code>frac</code> (fração dos exemplos da base usado para treinar o <a href="https://github.com/scikit-learn-contrib/boruta_py"><code>boruta.BorutaPy</code></a>) vemos quais variáveis estão sendo escolhidas. Idealmente, o <a href="https://github.com/scikit-learn-contrib/boruta_py"><code>boruta.BorutaPy</code></a> deveria conseguir identificar que as primeiras 50 variáveis (eixo x) são as úteis e selecioná-las (pintando-as de verde), enquanto exclui as 50 demais (pintando-as de azul), haja vista que são ruído. Conforme variamos o <code>frac</code> (eixo y), vemos como ele se comporta.</div></p>

```python
plot_heatmap(dic_sample, matrix)
```

![png](Sele%C3%A7%C3%A3o%20de%20vari%C3%A1veis%3A%20uma%20utiliza%C3%A7%C3%A3o%20cr%C3%ADtica%20do%20Boruta_files/output_38_0.png)

<p><div align="justify">Na primeira figura abaixo, vemos uma sumarização do plot anterior variando o <code>frac</code> (eixo x), enquanto observamos a porcentagem das variáveis úteis (em verde) e inúteis (em laranja) que são escolhidas. No gráfico ao lado, há uma análise de tempo (de treinamento do <a href="https://github.com/scikit-learn-contrib/boruta_py"><code>boruta.BorutaPy</code></a>) e performance do modelo treinado com as variáveis escolhidas naquele valor de <code>frac</code>.</div></p>

```python
plot_percentage_time(dic_sample, matrix, X_big, y_big)
```

![png](Sele%C3%A7%C3%A3o%20de%20vari%C3%A1veis%3A%20uma%20utiliza%C3%A7%C3%A3o%20cr%C3%ADtica%20do%20Boruta_files/output_40_0.png)

<p><div align="justify">Como podemos ver, não precisamos de todas as amostras para treinar o nosso <a href="https://github.com/scikit-learn-contrib/boruta_py"><code>boruta.BorutaPy</code></a>. No exemplo anterior, apesar da nossa amostra ter 5000 elementos, com algo em torno de 3000 exemplos, já era possível encontrar perfeitamente todas as 50 variáveis úteis para o nosso problema.</div></p>

<p><div align="justify">Na minha experiência utilizando o <a href="https://github.com/scikit-learn-contrib/boruta_py"><code>boruta.BorutaPy</code></a>, me sinto confortável com _uma amostra com 15 vezes mais exemplos do que features (ou seja, <code>n_samples&gt;=15*n_features</code>)_. Nesse limiar, já costumo ter resultados bons em termos de seleção de variáveis e é possível rodar o algoritmo (em tempo satisfatório para desenvolvimento) com um <code>max_depth</code> controlado. Colocando um exemplo numérico: se, no <a href="https://br.linkedin.com/showcase/serasa-experian-datalab">DataLab</a>, estou trabalhando com um problema de 5 mil variáveis, me sinto confortável em rodar o <a href="https://github.com/scikit-learn-contrib/boruta_py"><code>boruta.BorutaPy</code></a> em uma amostra de 75 mil linhas, mesmo tendo muito mais exemplos na base de desenvolvimento.</div></p>

<p><div align="justify">Por outro lado, o exemplo anterior nos mostra que nem sempre isso é o melhor, mesmo em questão de tempo. O <a href="https://github.com/scikit-learn-contrib/boruta_py"><code>boruta.BorutaPy</code></a>, na prática, não vai rodar por <code>max_iter</code> se já tiver certeza (no nível de significância <code>alpha</code>) das variáveis que ele acha úteis para o problema, que ele já exclui (ou seleciona) no meio do caminho. No experimento anterior, ter mais exemplos, na verdade, fez com que o <a href="https://github.com/scikit-learn-contrib/boruta_py"><code>boruta.BorutaPy</code></a> ficasse com mais certeza de forma mais rápida sobre as variáveis. Na prática, isso dificilmente acontece.</div></p>

# Usando o Boruta na prática e algumas alternativas

<p><div align="justify">As ideias por trás do <a href="https://github.com/scikit-learn-contrib/boruta_py"><code>boruta.BorutaPy</code></a> são muito interessantes, mas o algoritmo final é temporalmente custoso. Por sorte, podemos utilizar as ideias da construção para fazer variações espertas que podem ser alternativas se uma rodada inicial (com <code>max_depth ~ 10</code>, <code>perc=90</code> e <code>n_estimators=500</code>) estiver demorando demais:</div></p>
1. <p><div align="justify">Utilizar o <a href="https://github.com/vitaliset/blog-notebooks/blob/main/DataLab_Blog_Boruta_2022_09_05/selectktop_selector.py"><code>SelectKTop</code></a> com alguma métrica de <code>.feature_importances_</code> mais robusta (como o SHAP, usando algo como nosso <a href="https://github.com/vitaliset/blog-notebooks/blob/main/DataLab_Blog_Boruta_2022_09_05/shap_feature_importances_.py"><code>SHAPImportanceRandomForestClassifier</code></a>) e tendo cuidado com a escolha do <code>K</code>;</div></p>
2. <p><div align="justify">Adaptar o <a href="https://github.com/vitaliset/blog-notebooks/blob/main/DataLab_Blog_Boruta_2022_09_05/selectktop_selector.py"><code>SelectKTop</code></a> que construímos para um versão ainda mais robusta que lida com uma distribuição de <code>.feature_importances_</code> ao invés de apenas um estimador (aliás, esse é um ótimo exercício para o leitor interessado em entender melhor a <a href="https://scikit-learn.org/stable/developers/develop.html">API do scikit-learn</a>);</div></p>
3. <p><div align="justify">Adaptar o <a href="https://github.com/vitaliset/blog-notebooks/blob/main/DataLab_Blog_Boruta_2022_09_05/selectktop_selector.py"><code>SelectKTop</code></a> para um &quot;<code>SelectAboveNoise</code>&quot;, que explicamos anteriormente, criando as variáveis aleatórias a partir do <a href="https://numpy.org/doc/stable/reference/random/index.html"><code>numpy.random</code></a> (outro exercício muito bom);</div></p>
4. <p><div align="justify">Utilizar o <a href="https://github.com/scikit-learn-contrib/boruta_py"><code>boruta.BorutaPy</code></a> com algoritmos mais rápidos (como <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html"><code>sklearn.ensemble.ExtraTreesClassifier</code></a>), mas lembrando que seu treinamento (ainda mais randomizado) vai afetar o <code>.feature_importances_</code> e, consequentemente, o resultado final.</div></p>
5. <p><div align="justify">Reduzir a amostra utilizada para treino do <a href="https://github.com/scikit-learn-contrib/boruta_py"><code>boruta.BorutaPy</code></a> respeitando a <em>rule of thumb</em> de <code>n_samples&gt;=15*n_features</code>.</div></p>
6. <p><div align="justify">Mexer mais estruturalmente no algoritmo de forma que ele crie menos variáveis <em>shadows</em> em problemas com muitas variáveis (<em>to be tested</em>).</div></p>

<p><div align="justify">Se o seu problema é razoavelmente pequeno, usar o <a href="https://github.com/scikit-learn-contrib/boruta_py"><code>boruta.BorutaPy</code></a> com o SHAP e otimizar os hiper-parâmetros do <a href="https://github.com/scikit-learn-contrib/boruta_py"><code>boruta.BorutaPy</code></a> é uma boa opção. Para isso, será útil utilizar o <a href="https://github.com/vitaliset/blog-notebooks/blob/main/DataLab_Blog_Boruta_2022_09_05/boruta_selector.py"><code>Boruta</code></a> que criei no arquivo <a href="https://github.com/vitaliset/blog-notebooks/blob/main/DataLab_Blog_Boruta_2022_09_05/boruta_selector.py"><code>boruta_selector.py</code></a> no <a href="https://github.com/vitaliset/blog-notebooks/tree/main/DataLab_Blog_Boruta_2022_09_05">repositório deste post</a>. Ele já está no formato adequado de <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectorMixin.html"><code>Selector</code></a> do scikit-learn e pode ser utilizado da mesma forma que vimos o <a href="https://github.com/vitaliset/blog-notebooks/blob/main/DataLab_Blog_Boruta_2022_09_05/selectktop_selector.py"><code>SelectKTop</code></a> sendo usado (com o pipelines e qualquer <a href="https://github.com/scikit-learn/scikit-learn/blob/36958fb240fbe435673a9e3c52e769f01f36bec0/sklearn/model_selection/_search.py#L372"><code>BaseSearchCV</code></a> do scikit-learn).</div></p>

# Conclusão

<p><div align="justify">Seleção de variáveis é um assunto necessário quando queremos garantir ter um modelo robusto. Neste post vimos uma das técnicas mais úteis para abordar esse problema enquanto, ao entender suas ideias, discutimos como adaptá-la para uma variedade de casos específicos. Mesmo que você não consiga usar o Boruta no seu problema em questão, as ideias aqui expostas permitem que você faça uma seleção de variáveis sabendo melhor as falhas e os benefícios de abordagens usuais do mercado.</div></p>


# <a name="bibliography">Referências</a>

<p><div align="justify">[1] <a href="https://cs.nyu.edu/~mohri/mlbook/">Foundations of Machine Learning. Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. MIT Press, Second Edition, 2018</a>.</div></p>

<p><div align="justify">[2] <a href="https://www.jstatsoft.org/article/view/v036i11">Feature Selection with the Boruta Package. Miron B. Kursa, Witold R. Rudnicki. Journal of Statistical Software</a>.</div></p>

<p><div align="justify">[3] <a href="https://youtu.be/_L39rN6gz7Y">Decision and Classification Trees, Clearly Explained!!!. Josh Starmer. StatQuest with Josh Starmer</a>.</div></p>

<p><div align="justify">[4] <a href="https://towardsdatascience.com/boruta-explained-the-way-i-wish-someone-explained-it-to-me-4489d70e154a">Boruta Explained Exactly How You Wished Someone Explained to You. Samuele Mazzanti. Towards Data Science</a>.</div></p>

<p><div align="justify">[5] <a href="https://github.com/scikit-learn-contrib/boruta_py">boruta_py README.md documentation. Daniel Homola</a>.</div></p>

<p><div align="justify">Para mais dicas práticas de uso (e com um argumento de autoridade muito melhor que o meu), o autor do Boruta tem o guia <a href="https://cran.r-project.org/web/packages/Boruta/vignettes/inahurry.pdf">Boruta for those in a hurry</a> que, apesar de estar escrito em R, tem dicas práticas interessantes de alguém que conhece a implementação com muita profundidade.</div></p>

___